{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a96017a",
   "metadata": {},
   "source": [
    "# \"THE PRICE IS RIGHT\" Capstone Project\n",
    "\n",
    "This week - build a model that predicts how much something costs from a description, based on a scrape of Amazon data\n",
    "\n",
    "\n",
    "A model that can estimate how much something costs, from its description.\n",
    "\n",
    "# Order of play\n",
    "\n",
    "DAY 1: Data Curation  \n",
    "DAY 2: Data Pre-processing  \n",
    "DAY 3: Evaluation, Baselines, Traditional ML  \n",
    "DAY 4: Deep Learning and LLMs  \n",
    "DAY 5: Fine-tuning a Frontier Model  \n",
    "\n",
    "## DAY 2: Data Pre-processing\n",
    "\n",
    "Today we'll rewrite the products into a standard format.  \n",
    "LLMs are great at this!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ab97c",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business value of Data Pre-processing / Re-writing</h2>\n",
    "            <span style=\"color:#181;\">LLMs have made it simple to do something that was considered impossible only a few years ago.\n",
    "            This approach can be applied to almost any business vertical, and it's similar to the advanced techniques\n",
    "            we used on Week 5.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f562f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from pricer.batch import Batch\n",
    "from pricer.items import Item\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a59cb4",
   "metadata": {},
   "source": [
    "# The next cell is where you choose Dataset\n",
    "\n",
    "Use `LITE_MODE = True` for the free, fast version with training data size of 20,000\n",
    "\n",
    "USe `LITE_MODE =  False` for the powerful, full version with training data size of 800,000\n",
    "\n",
    "## For this lab\n",
    "\n",
    "You can skip altogether and load the dataset from HuggingFace: $0\n",
    "\n",
    "You can run pre-processing for the lite dataset: under $1\n",
    "\n",
    "You can run pre-processing for the full dataset: $30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f5c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "LITE_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede29b14",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'vince-dev/items_raw_lite' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDatasetNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m username = \u001b[33m\"\u001b[39m\u001b[33mvince-dev\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m dataset = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/items_raw_lite\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m LITE_MODE \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/items_raw_full\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train, val, test = \u001b[43mItem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m items = train + val + test\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(items)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m items\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I-Interim\\Desktop\\Dev\\AI ENGINEER COURSE ED DONNER\\llm_engineering\\week6\\pricer\\items.py:47\u001b[39m, in \u001b[36mItem.from_hub\u001b[39m\u001b[34m(cls, dataset_name)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_hub\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dataset_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Self], \u001b[38;5;28mlist\u001b[39m[Self], \u001b[38;5;28mlist\u001b[39m[Self]]:\n\u001b[32m     46\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load from HuggingFace Hub and reconstruct Items\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     ds = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     49\u001b[39m         [\u001b[38;5;28mcls\u001b[39m.model_validate(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m ds[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m     50\u001b[39m         [\u001b[38;5;28mcls\u001b[39m.model_validate(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m ds[\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m     51\u001b[39m         [\u001b[38;5;28mcls\u001b[39m.model_validate(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m ds[\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m     52\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I-Interim\\Desktop\\Dev\\AI ENGINEER COURSE ED DONNER\\llm_engineering\\.venv\\Lib\\site-packages\\datasets\\load.py:2062\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2057\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2058\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2059\u001b[39m )\n\u001b[32m   2061\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I-Interim\\Desktop\\Dev\\AI ENGINEER COURSE ED DONNER\\llm_engineering\\.venv\\Lib\\site-packages\\datasets\\load.py:1782\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1780\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   1781\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m1782\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   1795\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I-Interim\\Desktop\\Dev\\AI ENGINEER COURSE ED DONNER\\llm_engineering\\.venv\\Lib\\site-packages\\datasets\\load.py:1652\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1650\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1651\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[32m-> \u001b[39m\u001b[32m1652\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1654\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\I-Interim\\Desktop\\Dev\\AI ENGINEER COURSE ED DONNER\\llm_engineering\\.venv\\Lib\\site-packages\\datasets\\load.py:1578\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1574\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[32m   1575\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRevision \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1576\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1577\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1579\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1580\u001b[39m     dataset_script_path = api.hf_hub_download(\n\u001b[32m   1581\u001b[39m         repo_id=path,\n\u001b[32m   1582\u001b[39m         filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1585\u001b[39m         proxies=download_config.proxies,\n\u001b[32m   1586\u001b[39m     )\n",
      "\u001b[31mDatasetNotFoundError\u001b[39m: Dataset 'vince-dev/items_raw_lite' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": [
    "username = \"Dev75667\"\n",
    "dataset = f\"{username}/items_raw_lite\" if LITE_MODE else f\"{username}/items_raw_full\"\n",
    "\n",
    "train, val, test = Item.from_hub(dataset)\n",
    "\n",
    "items = train + val + test\n",
    "\n",
    "print(f\"Loaded {len(items):,} items\")\n",
    "print(items[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "items[2].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37158a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give every item an id\n",
    "\n",
    "for index, item in enumerate(items):\n",
    "    item.id = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Create a concise description of a product. Respond only in this format. Do not include part numbers.\n",
    "Title: Rewritten short precise title\n",
    "Category: eg Electronics\n",
    "Brand: Brand name\n",
    "Description: 1 sentence description\n",
    "Details: 1 sentence on features\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad12c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items[0].full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": items[0].full}]\n",
    "response = completion(messages=messages, model=\"groq/openai/gpt-oss-20b\", reasoning_effort=\"low\")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print()\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cost: {response._hidden_params['response_cost']*100:.3f} cents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": items[0].full}]\n",
    "response = completion(messages=messages, model=\"ollama/llama3.2\", api_base=\"http://localhost:11434\")\n",
    "print(response.choices[0].message.content)\n",
    "print()\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cost: {response._hidden_params['response_cost']*100:.3f} cents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4749a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"openai/gpt-oss-20b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_jsonl(item):\n",
    "    body = {\"model\": MODEL, \"messages\": [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": item.full}], \"reasoning_effort\": \"low\"}\n",
    "    line = {\"custom_id\": str(item.id), \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": body}\n",
    "    return json.dumps(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d49d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_jsonl(items[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d3e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_file(start, end, filename):\n",
    "    batch_file = filename\n",
    "    with open(batch_file, \"w\") as f:\n",
    "        for i in range(start, end):\n",
    "            f.write(make_jsonl(items[i]))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_file(0, 1000, \"jsonl/0_1000.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346babf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "groq = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf887de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"jsonl/0_1000.jsonl\", \"rb\") as f:\n",
    "    response = groq.files.create(file=f, purpose=\"batch\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = response.id\n",
    "file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = groq.batches.create(completion_window=\"24h\", endpoint=\"/v1/chat/completions\", input_file_id=file_id)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdedac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = groq.batches.retrieve(response.id)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a2de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = groq.files.content(result.output_file_id)\n",
    "response.write_to_file(\"jsonl/batch_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63094046",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"jsonl/batch_results.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        json_line = json.loads(line)\n",
    "        id = int(json_line[\"custom_id\"])\n",
    "        summary = json_line[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        items[id].summary = summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1974a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items[0].full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items[1000].summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708491a",
   "metadata": {},
   "source": [
    "## I've put exactly this logic into a Batch class\n",
    "\n",
    "- Divides items into groups of 1,000\n",
    "- Kicks off batches for each\n",
    "- Allows us to monitor and collect the results when complete\n",
    "\n",
    "## COSTS\n",
    "\n",
    "Using Groq, for me - this cost under $1 for the Lite dataset and under $30 for the big dataset\n",
    "\n",
    "But you don't need to pay anything! In the next lab, you can load my pre-processed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921381c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch.create(items, LITE_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea268775",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(items):\n",
    "    if not item.summary:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items[10234].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the fields that we don't need in the hub\n",
    "\n",
    "for item in items:\n",
    "    item.full = None\n",
    "    item.id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20533d23",
   "metadata": {},
   "source": [
    "## Push the final dataset to the hub\n",
    "\n",
    "If lite mode, we'll only push the lite dataset\n",
    "\n",
    "If full mode, we'll push both datasets (in case you decide to use lite later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"ed-donner\"\n",
    "full = f\"{username}/items_full\"\n",
    "lite = f\"{username}/items_lite\"\n",
    "\n",
    "if LITE_MODE:\n",
    "    train = items[:20_000]\n",
    "    val = items[20_000:21_000]\n",
    "    test = items[21_000:]\n",
    "    Item.push_to_hub(lite, train, val, test)\n",
    "else:\n",
    "    train = items[:800_000]\n",
    "    val = items[800_000:810_000]\n",
    "    test = items[810_000:]\n",
    "    Item.push_to_hub(full, train, val, test)\n",
    "\n",
    "    train_lite = train[:20_000]\n",
    "    val_lite = val[:1_000]\n",
    "    test_lite = test[:1_000]\n",
    "    Item.push_to_hub(lite, train_lite, val_lite, test_lite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb25f4",
   "metadata": {},
   "source": [
    "## And here they are!\n",
    "\n",
    "https://huggingface.co/datasets/ed-donner/items_lite\n",
    "\n",
    "https://huggingface.co/datasets/ed-donner/items_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ad0d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
